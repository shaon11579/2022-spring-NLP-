{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled44.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPXkdzDBYKni0IH/S2xiEwP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaon11579/2022-spring-NLP-/blob/main/June%2024_topic%20clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
      ],
      "metadata": {
        "id": "7_GhRumYPT6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA in Python – How to grid search best topic models?"
      ],
      "metadata": {
        "id": "Pu4bEnrLwOF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries for visualization\n",
        "!pip install pyLDAvis\n",
        "!pip install pyLDAvis.gensim_models\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOZ8txgxF4Q",
        "outputId": "c75f9faa-b4fb-419c-e617-77030c1c366b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=dc838ec4754d748aa67221864ba90265f1db072af59c658612ed5a78f890b6c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pyLDAvis.gensim_models (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pyLDAvis.gensim_models\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
            "  from numpy.dual import register_func\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNgpN0CBwFzz",
        "outputId": "40c9c917-0f86-4630-ec5e-5617c62258f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from .mio5_utils import VarReader5\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
            "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
            "  _deprecated()\n"
          ]
        }
      ],
      "source": [
        "# Run in terminal or command prompt\n",
        "# python3 -m spacy download en\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "df = pd.read_fwf(r'/bin/cleaned_file.txt', header=None)\n",
        "df = pd.DataFrame(df)\n",
        "#df = pd.read_json('/content/.config/cleaned_file.txt')\n",
        "#print(df.target_names.unique())\n",
        "df.head(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "1xPHl8GUxNhR",
        "outputId": "2acca65c-a9ed-4bc3-9411-3c5f13212dde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0\n",
              "0        IndustrialMarketingManagement89(2020)630–641\n",
              "1           Contents lists available at ScienceDirect\n",
              "2                     Industrial Marketing Management\n",
              "3   journal homepage: www.elsevier.com/locate/indm...\n",
              "4                                      Research paper\n",
              "5   B2B brands on Twitter: Engaging rs with a vary...\n",
              "6                 objectives, strategies, and tactics\n",
              "7   Mari Juntunenᵃ,⁎, Elvira Ismagilovaᵇ, Eeva-Lii...\n",
              "8   ᵃ Department of Marketing, Management and Inte...\n",
              "9              of Oulu, P.O. Box 4600, 90014, Finland\n",
              "10  ᵇ School of Management, University of Bradford...\n",
              "11  ᶜ Department of Marketing, Management and Inte...\n",
              "12                              A R T I C L E I N F O\n",
              "13                                          Keywords:\n",
              "14                                    B2B advertising"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f71fb3c2-a3b3-4e8b-865b-f6a576d190fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndustrialMarketingManagement89(2020)630–641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Contents lists available at ScienceDirect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Industrial Marketing Management</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>journal homepage: www.elsevier.com/locate/indm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Research paper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>B2B brands on Twitter: Engaging rs with a vary...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>objectives, strategies, and tactics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mari Juntunenᵃ,⁎, Elvira Ismagilovaᵇ, Eeva-Lii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ᵃ Department of Marketing, Management and Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>of Oulu, P.O. Box 4600, 90014, Finland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ᵇ School of Management, University of Bradford...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ᶜ Department of Marketing, Management and Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A R T I C L E I N F O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Keywords:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>B2B advertising</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f71fb3c2-a3b3-4e8b-865b-f6a576d190fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f71fb3c2-a3b3-4e8b-865b-f6a576d190fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f71fb3c2-a3b3-4e8b-865b-f6a576d190fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "zAdqF2XH-kld"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to list\n",
        "df = df.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "#df = [re.sub('\\S*@\\S*\\s?', '', str(df)) for sent in df]\n",
        "\n",
        "# Remove new line characters\n",
        "#df = [re.sub('\\s+', ' ', sent) for sent in df]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "#df = [re.sub(\"\\'\", \"\", sent) for sent in df]\n",
        "\n",
        "pprint(df[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqx2fBnN0vyk",
        "outputId": "8a7d7de5-9582-4e28-ed8c-013bfc365403"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['IndustrialMarketingManagement89(2020)630–641']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pprint(df[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cErn-E4XTYpc",
        "outputId": "602c71ea-648f-4885-fed4-bc394529ef67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(df))\n",
        "\n",
        "print(data_words[:10])"
      ],
      "metadata": {
        "id": "7_RPJEHBCLNy",
        "outputId": "00ae44e3-acff-4a03-f641-5b54f397dda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], ['contents', 'lists', 'available', 'at', 'sciencedirect'], ['industrial', 'marketing', 'management'], ['journal', 'homepage', 'www', 'elsevier', 'com', 'locate', 'indmarman'], ['research', 'paper'], ['brands', 'on', 'twitter', 'engaging', 'rs', 'with', 'varying', 'combination', 'of', 'social', 'media', 'content'], ['objectives', 'strategies', 'and', 'tactics'], ['mari', 'juntunenᵃ', 'elvira', 'ismagilovaᵇ', 'eeva', 'liisa', 'oikarinenᶜ'], ['department', 'of', 'marketing', 'management', 'and', 'international', 'business', 'oulu', 'business', 'school', 'university'], ['of', 'oulu', 'box', 'finland']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "SiaTexKcTvaj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Lemmatization\n",
        "\n",
        " Lemmatization is a process where we convert words to its root word.\n",
        "For example: ‘Studying’ becomes ‘Study’, ‘Meeting becomes ‘Meet’, ‘Better’ and ‘Best’ becomes ‘Good’.\n",
        "\n",
        "The advantage of this is, we get to reduce the total number of unique words in the dictionary.\n",
        "\n",
        "As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step) will be denser with lesser columns.\n",
        "\n",
        "You can expect better topics to be generated in the end."
      ],
      "metadata": {
        "id": "Q__r6QJFeviN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
        "    return texts_out\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# Run in terminal: python3 -m spacy download en\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
        "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:10])"
      ],
      "metadata": {
        "id": "nXPGOJ6lV2Yc",
        "outputId": "125fdc4b-fea0-44ed-e27c-c0db08967026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'content list available sciencedirect', 'industrial marketing management', 'homepage elsevi com locate indmarman', 'research paper', 'brand twitter engage vary combination social medium content', 'objective strategy tactic', 'oikarinenᶜ', 'marketing management international business oulu business school university', 'box finland']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Document-Word matrix"
      ],
      "metadata": {
        "id": "n6Ck9LRCf5tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer='word',       \n",
        "                             min_df=10,                        # minimum reqd occurences of a word \n",
        "                             stop_words='english',             # remove stop words\n",
        "                             lowercase=True,                   # convert all words to lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                             # max_features=50000,             # max number of uniq words\n",
        "                            )\n",
        "\n",
        "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
      ],
      "metadata": {
        "id": "-N627OWCfvBD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Check the Sparsicity"
      ],
      "metadata": {
        "id": "_wfUgl411Dn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Materialize the sparse data\n",
        "data_dense = data_vectorized.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "metadata": {
        "id": "DbBTGWBL1BKX",
        "outputId": "e40f4b1c-4bf7-496f-abda-e61817ad02ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e792e5c63e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Materialize the sparse data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_vectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute Sparsicity = Percentage of Non-Zero cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sparsicity: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dense\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_vectorized' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. # Build LDA model with sklearn\n",
        "\n",
        "Everything is ready to build a Latent Dirichlet Allocation (LDA) model. Let’s initialise one and call fit_transform() to build the LDA model.\n",
        "\n",
        "For this example, I have set the n_topics as 20 based on prior knowledge about the dataset. Later we will find the optimal number using grid search."
      ],
      "metadata": {
        "id": "IkjYFTPB1O9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA Model\n",
        "lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics\n",
        "                                      max_iter=10,               # Max learning iterations\n",
        "                                      learning_method='online',   \n",
        "                                      random_state=100,          # Random state\n",
        "                                      batch_size=128,            # n docs in each learning iter\n",
        "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
        "                                      n_jobs = -1,\n",
        "                                      )              # Use all available CPUs\n",
        "                                      \n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n",
        "\n",
        "print(lda_model)  # Model attributes"
      ],
      "metadata": {
        "id": "lDwGGkc71LLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
        "             evaluate_every=-1, learning_decay=0.7,\n",
        "             learning_method='online', learning_offset=10.0,\n",
        "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
        "             n_components=20, n_jobs=-1, perp_tol=0.1,\n",
        "             random_state=100, topic_word_prior=None,\n",
        "             total_samples=1000000.0, verbose=0)"
      ],
      "metadata": {
        "id": "35o9tJaaO6Vr",
        "outputId": "954db55b-e836-4095-b4fe-efc67d2ef1d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(learning_method='online', n_components=20, n_jobs=-1,\n",
              "                          random_state=100)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Diagnose model performance with perplexity and log-likelihood"
      ],
      "metadata": {
        "id": "4r8Q25bfPBhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
        "\n",
        "# See model parameters\n",
        "pprint(lda_model.get_params())"
      ],
      "metadata": {
        "id": "KmdWRtUqO-rE",
        "outputId": "f8e629e9-a504-4771-9789-0f668e405615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Likelihood:  -1994206.0587655604\n",
            "Perplexity:  2360.784369196808\n",
            "{'batch_size': 128,\n",
            " 'doc_topic_prior': None,\n",
            " 'evaluate_every': -1,\n",
            " 'learning_decay': 0.7,\n",
            " 'learning_method': 'online',\n",
            " 'learning_offset': 10.0,\n",
            " 'max_doc_update_iter': 100,\n",
            " 'max_iter': 10,\n",
            " 'mean_change_tol': 0.001,\n",
            " 'n_components': 20,\n",
            " 'n_jobs': -1,\n",
            " 'perp_tol': 0.1,\n",
            " 'random_state': 100,\n",
            " 'topic_word_prior': None,\n",
            " 'total_samples': 1000000.0,\n",
            " 'verbose': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. How to GridSearch the best LDA model?\n",
        ".\n",
        "The most important tuning parameter for LDA models is n_components (number of topics). In addition, I am going to search learning_decay (which controls the learning rate) as well.\n",
        "\n",
        "Besides these, other possible search params could be learning_offset (downweigh early iterations. Should be > 1) and max_iter. These could be worth experimenting if you have enough computing resources.\n",
        "\n",
        "Be warned, the grid search constructs multiple LDA models for all possible combinations of param values in the param_grid dict. So, this process can consume a lot of time and resources."
      ],
      "metadata": {
        "id": "eYFH2Wr5PSXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "metadata": {
        "id": "rUsTkHhgPIBl",
        "outputId": "517940c0-7b1c-433e-8cef-7d0f9cd0233a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
              "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
              "                         'n_components': [10, 15, 20, 25, 30]})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/darenr/scikit-optimize"
      ],
      "metadata": {
        "id": "a99mV6DwSTpw",
        "outputId": "db0802d1-a8a0-4c69-c910-e4a05e865dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/darenr/scikit-optimize\n",
            "  Cloning https://github.com/darenr/scikit-optimize to /tmp/pip-req-build-gs6jrw8_\n",
            "  Running command git clone -q https://github.com/darenr/scikit-optimize /tmp/pip-req-build-gs6jrw8_\n",
            "Collecting pyaml\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.6+19.g180d6be) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.6+19.g180d6be) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml->scikit-optimize==0.6+19.g180d6be) (3.13)\n",
            "Building wheels for collected packages: scikit-optimize\n",
            "  Building wheel for scikit-optimize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-optimize: filename=scikit_optimize-0.6+19.g180d6be-py2.py3-none-any.whl size=75418 sha256=fad3c6e31f5947b1201c51306c7a3f0ac6a9eadc8ea898078ed2c747d099d3b5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xdlr8d_s/wheels/11/4c/bf/aa91b51947ed5367f98d5a68c59afd45a275b85e9fc4827007\n",
            "Successfully built scikit-optimize\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.10.1 scikit-optimize-0.6+19.g180d6be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV(cv=None, error_score='raise',\n",
        "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
        "             evaluate_every=-1, learning_decay=0.7, learning_method='batch',\n",
        "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
        "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
        "             perp_tol=0.1, random_state=None,\n",
        "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
        "       fit_params=None, iid=True, n_jobs=1,\n",
        "       param_grid={'n_components': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
        "       scoring=None, verbose=0)"
      ],
      "metadata": {
        "id": "o8QSMLpkPl_l",
        "outputId": "b17cfa71-a2d8-4c93-a0a4-f775fb843610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-1864c053beaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_components'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m        \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2*n_jobs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m        scoring=None, verbose=0)\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'fit_params'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. How to see the best topic model and its parameters?"
      ],
      "metadata": {
        "id": "BjPmOUTUPn09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "metadata": {
        "id": "KiihYT78PrBD",
        "outputId": "dfd1a860-fb92-434e-ecac-c8bbb6d6dc2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 10}\n",
            "Best Log Likelihood Score:  -433184.0310602117\n",
            "Model Perplexity:  1539.6152932086657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Compare LDA Model Performance Scores"
      ],
      "metadata": {
        "id": "I6i59T0cP2AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Log Likelyhoods from Grid Search Output\n",
        "n_topics = [10, 15, 20, 25, 30]\n",
        "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]\n",
        "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
        "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
        "\n",
        "# Show graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
        "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
        "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
        "plt.title(\"Choosing Optimal LDA Model\")\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Log Likelyhood Scores\")\n",
        "plt.legend(title='Learning decay', loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qKsOpkysPvJE",
        "outputId": "2d940151-6f10-413f-f5cd-bc9b2d6f5fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e252b50aa003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Log Likelyhoods from Grid Search Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_likelyhoods_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlog_likelyhoods_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_likelyhoods_9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid_scores_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. How to see the dominant topic in each document?"
      ],
      "metadata": {
        "id": "NpYlzOcHQAf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Document - Topic Matrix\n",
        "lda_output = best_lda_model.transform(data_vectorized)\n",
        "\n",
        "# column names\n",
        "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_topics)]\n",
        "\n",
        "# index names\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
        "\n",
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "# Styling\n",
        "def color_green(val):\n",
        "    color = 'green' if val > .1 else 'black'\n",
        "    return 'color: {col}'.format(col=color)\n",
        "\n",
        "def make_bold(val):\n",
        "    weight = 700 if val > .1 else 400\n",
        "    return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n",
        "# Apply Style\n",
        "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics"
      ],
      "metadata": {
        "id": "6prPcdliP8f5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}