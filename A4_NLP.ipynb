{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4_NLP.ipynb",
      "provenance": [],
      "mount_file_id": "1YRLYghULA5V9KIahAJdpghSEx_V3C1ki",
      "authorship_tag": "ABX9TyPGQCt5cAFLILNmvvsqzWum",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaon11579/2022-spring-NLP-/blob/main/A4_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nyjnkNf-O0a9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.tree import *\n",
        "import numpy as np\n",
        "from assignment2 import generate_clean_file,get_words,get_hash_of_word_pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def check_for_punc(x):\n",
        "    temp  = True\n",
        "    for c in x:\n",
        "        if c != \" \":\n",
        "            temp = np.logical_and(temp,c in string.punctuation)\n",
        "    if temp:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def generate_clean_file(input,output):\n",
        "    file = open(input)\n",
        "    lines = list(map(lambda x:x.split('('),file.readlines()))\n",
        "    lines = list(filter(lambda x: len(x)>1,lines))\n",
        "    lines = list(map(lambda x:x[-1].split(')')[0],lines))\n",
        "    lines = list(filter(lambda x:x.split()[0]!= '-NONE-',lines))\n",
        "    # lines = list(filter(lambda x:check_for_punc(x),lines))\n",
        "    lines = list(filter(lambda x: x.find('-LRB-') == -1,lines))\n",
        "    lines = list(filter(lambda x: x.find('-RRB-') == -1,lines))\n",
        "    ind = np.array([i for i in range(len(lines)) if lines[i].find('TOP END_OF_TEXT_UNIT') != -1])\n",
        "    lines = np.array(lines)\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(ind)-1):\n",
        "        sentences.append(\" \".join(lines[ind[i]+1:ind[i+1]]))\n",
        "    sentences.append(\" \".join(lines[ind[-1]+1:]))\n",
        "    out  = open(output,\"w\")\n",
        "    for line in sentences:\n",
        "        if len(line)>0:\n",
        "            out.write(line)\n",
        "            out.write('\\n')\n",
        "\n",
        "def get_words(outputfiles):\n",
        "    file = open(outputfiles)\n",
        "    lines = (\" \".join(list(map(lambda x:x.split(\"\\n\")[0],file.readlines())))).split(\" \")\n",
        "    lines = list(filter(lambda x:x!='',lines))\n",
        "    return lines\n",
        "\n",
        "\n",
        "\n",
        "def get_hash_of_word_pos(lines):\n",
        "    tuple_collection = [(lines[k],lines[k+1]) for k in range(0,len(lines)-1,2)]\n",
        "    temp = Counter(tuple_collection)\n",
        "    first_dict = {}\n",
        "    for key in temp.keys():\n",
        "        if key[0] not in first_dict.keys():\n",
        "            first_dict[key[0]] = {key[1]:temp[key]}\n",
        "        else:\n",
        "            first_dict[key[0]].update({key[1]:temp[key]})\n",
        "    return first_dict\n",
        "\n",
        "def report_20_most_frequent_from_brown(lines):\n",
        "    tuple_collection = [(lines[k],lines[k+1]) for k in range(0,len(lines)-1,2)]\n",
        "    temp = Counter(tuple_collection)\n",
        "    first_dict = {}\n",
        "    for key in temp.keys():\n",
        "        if key[0] not in first_dict.keys():\n",
        "            first_dict[key[0]] = {key[1]:temp[key]}\n",
        "        else:\n",
        "            first_dict[key[0]].update({key[1]:temp[key]})\n",
        "    hash_of_pos = first_dict\n",
        "    freq = {}\n",
        "    for key in hash_of_pos:\n",
        "        freq[key] = sum([int(hash_of_pos[key][key1]) for key1 in hash_of_pos[key].keys()])\n",
        "    print('The twenty most frequent tags are')\n",
        "    for u,v in Counter(freq).most_common(20):\n",
        "        print(u,v)\n",
        "\n",
        "def get_accuracy(lines,hash_of_word_pos):\n",
        "    tags = [lines[i] for i in range(0,len(lines)-1,2)]\n",
        "    words = [lines[i+1] for i in range(0,len(lines)-1,2)]\n",
        "    most_freq = []\n",
        "    match = 0\n",
        "    for i,word in enumerate(words):\n",
        "        most_freq.append(Counter(hash_of_word_pos[word]).most_common(1)[0][0])\n",
        "    for k in range(len(most_freq)):\n",
        "        if most_freq[k] == tags[k]:\n",
        "            match+=1\n",
        "    print(match,len(most_freq),\"Accuracy is \",100*match/len(most_freq),'%')\n",
        "\n",
        "# inputfiles = 'BROWN.pos.all'\n",
        "# outputfiles = 'BROWN-clean.pos-all.txt'\n",
        "# # Answer to (i): generate clean file\n",
        "# generate_clean_file(inputfiles,outputfiles)\n",
        "#\n",
        "# #read the output file=clean brown file\n",
        "# lines = get_words(outputfiles)\n",
        "#\n",
        "# # answer to (iii): report 20 most frequent tag\n",
        "# report_20_most_frequent_from_brown(lines)\n",
        "#\n",
        "# # answer to (ii): generate hash of hash\n",
        "# hash_of_word_pos = get_hash_of_word_pos(lines)\n",
        "#\n",
        "# # report accuracy\n",
        "# get_accuracy(lines,hash_of_word_pos)\n",
        "#\n"
      ],
      "metadata": {
        "id": "PWefmeZClgvo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputfiles = '/content/BROWN.pos.all.txt'\n",
        "outputfiles = '/content/BROWN-clean.pos.txt'"
      ],
      "metadata": {
        "id": "IKUPcAZ3oRDS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences(sentences_init):\n",
        "    sentences = sentences_init.split('(TOP')\n",
        "    sentence_collection = []\n",
        "    for sentence in sentences:\n",
        "        if sentence == '\\n\\n':\n",
        "            continue\n",
        "        temp = '(TOP'+ sentence\n",
        "        if temp.count(\"(\") != temp.count(\")\"):\n",
        "            sentence_collection.append('(TOP'+sentence+')')\n",
        "        else:\n",
        "            sentence_collection.append('(TOP'+sentence)\n",
        "    sentence_collection_final = []\n",
        "    for s in sentence_collection:\n",
        "        sentence_collection_final.append(s.split('\\n\\n')[0])\n",
        "    return sentence_collection\n",
        "\n"
      ],
      "metadata": {
        "id": "zCSoFvMVj3FZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences(sentences_init):\n",
        "    sentences = sentences_init.split('(TOP')\n",
        "    sentence_collection = []\n",
        "    for sentence in sentences:\n",
        "        if sentence == '\\n\\n':\n",
        "            continue\n",
        "        temp = '(TOP'+ sentence\n",
        "        if temp.count(\"(\") != temp.count(\")\"):\n",
        "            sentence_collection.append('(TOP'+sentence+')')\n",
        "        else:\n",
        "            sentence_collection.append('(TOP'+sentence)\n",
        "    sentence_collection_final = []\n",
        "    for s in sentence_collection:\n",
        "        sentence_collection_final.append(s.split('\\n\\n')[0])\n",
        "    return sentence_collection\n"
      ],
      "metadata": {
        "id": "mmuUR5ajkCp0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences(sentences_init):\n",
        "    sentences = sentences_init.split('(TOP')\n",
        "    sentence_collection = []\n",
        "    for sentence in sentences:\n",
        "        if sentence == '\\n\\n':\n",
        "            continue\n",
        "        temp = '(TOP'+ sentence\n",
        "        if temp.count(\"(\") != temp.count(\")\"):\n",
        "            sentence_collection.append('(TOP'+sentence+')')\n",
        "        else:\n",
        "            sentence_collection.append('(TOP'+sentence)\n",
        "    sentence_collection_final = []\n",
        "    for s in sentence_collection:\n",
        "        sentence_collection_final.append(s.split('\\n\\n')[0])\n",
        "    return sentence_collection\n"
      ],
      "metadata": {
        "id": "JZ5q60-OkF1s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_right(rule_collection):\n",
        "    final_right_rule_collection = []\n",
        "    for rule in rule_collection:\n",
        "        left,right = str(rule).split('->')\n",
        "        final_right_rule_collection.append(right.strip())\n",
        "    return final_right_rule_collection"
      ],
      "metadata": {
        "id": "bLZBWkaJkJRo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_diverse_left(unique_rule_collection):\n",
        "    final_left_terminal = []\n",
        "    for rule in unique_rule_collection:\n",
        "        left,right = str(rule).split('->')\n",
        "        final_left_terminal.append(left.strip())\n",
        "    return final_left_terminal"
      ],
      "metadata": {
        "id": "WyXu4XAskRn2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hash_of_pos(filename):\n",
        "    output = 'cleanfile'\n",
        "    generate_clean_file(filename,output)\n",
        "    lines = get_words(output)\n",
        "    hash_of_pos = get_hash_of_word_pos(lines)\n",
        "    return hash_of_pos"
      ],
      "metadata": {
        "id": "07upLjcAkWHv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lexicalized_grammar_length(rule_collection,hash_pos):\n",
        "    unique_rule_collection = np.unique(rule_collection)\n",
        "    count = 0\n",
        "    for rule in unique_rule_collection:\n",
        "        left,right = str(rule).split('->')\n",
        "        pos = [left.strip()] + list(map(lambda x:x.strip(),right.split()))\n",
        "        c = 1\n",
        "        for p in pos:\n",
        "            if p in list(hash_pos.keys()):\n",
        "                c = c*len(list(hash_pos[p].keys()))\n",
        "        count+=c\n",
        "    return count"
      ],
      "metadata": {
        "id": "slxJbD8Qkp_8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer to the question no 1\n",
        "filename = 'BROWN.pos.all'\n",
        "with open(filename) as f:\n",
        "    data = f.read()\n",
        "rule_collection = get_rule_collection(data)\n",
        "print(\"The number of unique rules found = \",len(np.unique(rule_collection)))\n",
        "final_right_rule_collection = get_right(rule_collection)\n",
        "print(\"The 10 most common rules regardless of the left non terminal = \",Counter(final_right_rule_collection).most_common(10))\n",
        "left_terminal_collection = get_most_diverse_left(np.unique(rule_collection))\n",
        "print(\"The non terminal with most diverse alternatives = \",Counter(left_terminal_collection).most_common(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "pwqSF1BwkvfQ",
        "outputId": "9b59f2b6-b943-4981-f715-ffb03a6d72aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e279527c4ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrule_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rule_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of unique rules found = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfinal_right_rule_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_rule_collection' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer to the question no 2\n",
        "hash_pos = get_hash_of_pos(filename)\n",
        "count = get_lexicalized_grammar_length(rule_collection,hash_pos)\n",
        "\n",
        "print(\"The grammar would be of length \",count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "zpAhbsxolwGT",
        "outputId": "c2b55c91-eab2-4f9b-c944-bfaddfcad15b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4e2a1bac2d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Answer to the question no 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhash_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hash_of_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lexicalized_grammar_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhash_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The grammar would be of length \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rule_collection' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "files = os.listdir()\n",
        "with open(\"combined_file.txt\", \"a\") as f:\n",
        "    for file_name in files:\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            with open(file_name) as f1:\n",
        "                f.write(f1.read())\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n"
      ],
      "metadata": {
        "id": "ccECrOIWlu3m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from io import StringIO\n",
        "\n",
        "def convert_pdf_to_txt(path):\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    codec = 'utf-8'\n",
        "    laparams = LAParams()\n",
        "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
        "    fp = open(path, 'rb')\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    password = \"\"\n",
        "    maxpages = 0\n",
        "    caching = True\n",
        "    pagenos=set()\n",
        "\n",
        "\n",
        "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "\n",
        "\n",
        "    text = retstr.getvalue()\n",
        "\n",
        "    fp.close()\n",
        "    device.close()\n",
        "    retstr.close()\n",
        "    return text\n",
        "\n",
        "text= convert_pdf_to_txt('test.pdf')\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "K7nIfqaWqbsR",
        "outputId": "c733caea-403f-4fdd-946c-57bb99a358bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a3d1991dd96e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfinterp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextConverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLAParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import collections\n",
        "pdf_file = open('samples.pdf', 'rb')\n",
        "read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
        "number_of_pages = read_pdf.getNumPages()\n",
        "c = collections.Counter(range(number_of_pages))\n",
        "for i in c:\n",
        "   page = read_pdf.getPage(i)\n",
        "   page_content = page.extractText()\n",
        "   print page_content.encode('utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "lWA9uJ7mp0tY",
        "outputId": "031a97c7-13c2-4eae-9bcc-5aba59d3d31a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-925782c26ecd>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print page_content.encode('utf-8')\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}