{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled44.ipynb",
      "provenance": [],
      "background_execution": "on",
      "authorship_tag": "ABX9TyNW0ma/J+1yfakiAx5L+vk0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaon11579/2022-spring-NLP-/blob/main/July%2031_topic%20clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
      ],
      "metadata": {
        "id": "7_GhRumYPT6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA in Python – How to grid search best topic models?"
      ],
      "metadata": {
        "id": "Pu4bEnrLwOF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries for visualization\n",
        "!pip install pyLDAvis\n",
        "!pip install pyLDAvis.gensim_models\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOZ8txgxF4Q",
        "outputId": "0955a4c6-e79f-4b41-9b1b-0595302722a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=11982b10a2dbbfc756fddd9899f1200497f78f9ed536c4e3e3124c9108c0b774\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pyLDAvis.gensim_models (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pyLDAvis.gensim_models\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zNgpN0CBwFzz",
        "outputId": "6de730ff-15f6-4bf9-bf68-c7ed02f93500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
            "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
            "  _deprecated()\n"
          ]
        }
      ],
      "source": [
        "# Run in terminal or command prompt\n",
        "# python3 -m spacy download en\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "df = pd.read_fwf(r'/bin/cleaned_file.txt', header=None)\n",
        "df = pd.DataFrame(df)\n",
        "#df = pd.read_json('/content/.config/cleaned_file.txt')\n",
        "#print(df.target_names.unique())\n",
        "df.head(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "1xPHl8GUxNhR",
        "outputId": "a904b3b9-b629-4aa9-edc5-ebb526d5933e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0\n",
              "0        IndustrialMarketingManagement89(2020)630–641\n",
              "1           Contents lists available at ScienceDirect\n",
              "2                     Industrial Marketing Management\n",
              "3   journal homepage: www.elsevier.com/locate/indm...\n",
              "4                                      Research paper\n",
              "5   B2B brands on Twitter: Engaging rs with a vary...\n",
              "6                 objectives, strategies, and tactics\n",
              "7   Mari Juntunenᵃ,⁎, Elvira Ismagilovaᵇ, Eeva-Lii...\n",
              "8   ᵃ Department of Marketing, Management and Inte...\n",
              "9              of Oulu, P.O. Box 4600, 90014, Finland\n",
              "10  ᵇ School of Management, University of Bradford...\n",
              "11  ᶜ Department of Marketing, Management and Inte...\n",
              "12                              A R T I C L E I N F O\n",
              "13                                          Keywords:\n",
              "14                                    B2B advertising"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63f126b7-3e8c-4df5-b138-ebffddc779c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndustrialMarketingManagement89(2020)630–641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Contents lists available at ScienceDirect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Industrial Marketing Management</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>journal homepage: www.elsevier.com/locate/indm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Research paper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>B2B brands on Twitter: Engaging rs with a vary...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>objectives, strategies, and tactics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mari Juntunenᵃ,⁎, Elvira Ismagilovaᵇ, Eeva-Lii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ᵃ Department of Marketing, Management and Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>of Oulu, P.O. Box 4600, 90014, Finland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ᵇ School of Management, University of Bradford...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ᶜ Department of Marketing, Management and Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A R T I C L E I N F O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Keywords:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>B2B advertising</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63f126b7-3e8c-4df5-b138-ebffddc779c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-63f126b7-3e8c-4df5-b138-ebffddc779c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-63f126b7-3e8c-4df5-b138-ebffddc779c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "zAdqF2XH-kld"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to list\n",
        "df = df.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "#df = [re.sub('\\S*@\\S*\\s?', '', str(df)) for sent in df]\n",
        "\n",
        "# Remove new line characters\n",
        "#df = [re.sub('\\s+', ' ', sent) for sent in df]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "#df = [re.sub(\"\\'\", \"\", sent) for sent in df]\n",
        "\n",
        "pprint(df[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqx2fBnN0vyk",
        "outputId": "ac83f7aa-252e-40b3-9e6a-00ff486832c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['IndustrialMarketingManagement89(2020)630–641']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pprint(df[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cErn-E4XTYpc",
        "outputId": "602c71ea-648f-4885-fed4-bc394529ef67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(df))\n",
        "\n",
        "print(data_words[:10])"
      ],
      "metadata": {
        "id": "7_RPJEHBCLNy",
        "outputId": "6b6806ca-344c-4686-fa97-8f186cea3351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], ['contents', 'lists', 'available', 'at', 'sciencedirect'], ['industrial', 'marketing', 'management'], ['journal', 'homepage', 'www', 'elsevier', 'com', 'locate', 'indmarman'], ['research', 'paper'], ['brands', 'on', 'twitter', 'engaging', 'rs', 'with', 'varying', 'combination', 'of', 'social', 'media', 'content'], ['objectives', 'strategies', 'and', 'tactics'], ['mari', 'juntunenᵃ', 'elvira', 'ismagilovaᵇ', 'eeva', 'liisa', 'oikarinenᶜ'], ['department', 'of', 'marketing', 'management', 'and', 'international', 'business', 'oulu', 'business', 'school', 'university'], ['of', 'oulu', 'box', 'finland']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Lemmatization\n",
        "\n",
        " Lemmatization is a process where we convert words to its root word.\n",
        "For example: ‘Studying’ becomes ‘Study’, ‘Meeting becomes ‘Meet’, ‘Better’ and ‘Best’ becomes ‘Good’.\n",
        "\n",
        "The advantage of this is, we get to reduce the total number of unique words in the dictionary.\n",
        "\n",
        "As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step) will be denser with lesser columns.\n",
        "\n",
        "You can expect better topics to be generated in the end."
      ],
      "metadata": {
        "id": "Q__r6QJFeviN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
        "    return texts_out\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# Run in terminal: python3 -m spacy download en\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
        "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:10])"
      ],
      "metadata": {
        "id": "nXPGOJ6lV2Yc",
        "outputId": "862a3012-2baf-4ab4-eebc-2cfae8e59f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'content list available sciencedirect', 'industrial marketing management', 'homepage www elsevi com locate indmarman', 'research paper', 'brand twitter engage vary combination social medium content', 'objective strategy tactic', '', 'marketing management international business oulu university', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Document-Word matrix"
      ],
      "metadata": {
        "id": "n6Ck9LRCf5tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer='word',       \n",
        "                             min_df=10,                        # minimum reqd occurences of a word \n",
        "                             stop_words='english',             # remove stop words\n",
        "                             lowercase=True,                   # convert all words to lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                             # max_features=50000,             # max number of uniq words\n",
        "                            )\n",
        "\n",
        "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
      ],
      "metadata": {
        "id": "-N627OWCfvBD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Check the Sparsicity"
      ],
      "metadata": {
        "id": "_wfUgl411Dn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Materialize the sparse data\n",
        "# \n",
        "#data_dense = data_vectorized.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "#print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "metadata": {
        "id": "DbBTGWBL1BKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. # Build LDA model with sklearn\n",
        "\n",
        "Everything is ready to build a Latent Dirichlet Allocation (LDA) model. Let’s initialise one and call fit_transform() to build the LDA model.\n",
        "\n",
        "For this example, I have set the n_topics as 20 based on prior knowledge about the dataset. Later we will find the optimal number using grid search."
      ],
      "metadata": {
        "id": "IkjYFTPB1O9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA Model\n",
        "lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics\n",
        "                                      max_iter=10,               # Max learning iterations\n",
        "                                      learning_method='online',   \n",
        "                                      random_state=100,          # Random state\n",
        "                                      batch_size=128,            # n docs in each learning iter\n",
        "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
        "                                      n_jobs = 1,\n",
        "                                      )              # Use all available CPUs\n",
        "                                      \n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n",
        "\n",
        "print(lda_model)  # Model attributes"
      ],
      "metadata": {
        "id": "lDwGGkc71LLw",
        "outputId": "23038cc9-9b58-413c-df1f-2e9fc6d1de35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LatentDirichletAllocation(learning_method='online', n_components=20, n_jobs=1,\n",
            "                          random_state=100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
        "             evaluate_every=-1, learning_decay=0.7,\n",
        "             learning_method='online', learning_offset=10.0,\n",
        "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
        "             n_components=20, n_jobs=-1, perp_tol=0.1,\n",
        "             random_state=100, topic_word_prior=None,\n",
        "             total_samples=1000000.0, verbose=0)"
      ],
      "metadata": {
        "id": "35o9tJaaO6Vr",
        "outputId": "8838a5be-2ce9-4d7f-b4f3-d808c029b5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(learning_method='online', n_components=20, n_jobs=-1,\n",
              "                          random_state=100)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Diagnose model performance with perplexity and log-likelihood"
      ],
      "metadata": {
        "id": "4r8Q25bfPBhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
        "\n",
        "# See model parameters\n",
        "pprint(lda_model.get_params())"
      ],
      "metadata": {
        "id": "KmdWRtUqO-rE",
        "outputId": "b0beb9c5-28d4-4c3d-cb9f-f3717a1e8974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Likelihood:  -17905321.13958816\n",
            "Perplexity:  3724.610623794759\n",
            "{'batch_size': 128,\n",
            " 'doc_topic_prior': None,\n",
            " 'evaluate_every': -1,\n",
            " 'learning_decay': 0.7,\n",
            " 'learning_method': 'online',\n",
            " 'learning_offset': 10.0,\n",
            " 'max_doc_update_iter': 100,\n",
            " 'max_iter': 10,\n",
            " 'mean_change_tol': 0.001,\n",
            " 'n_components': 20,\n",
            " 'n_jobs': 1,\n",
            " 'perp_tol': 0.1,\n",
            " 'random_state': 100,\n",
            " 'topic_word_prior': None,\n",
            " 'total_samples': 1000000.0,\n",
            " 'verbose': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. How to GridSearch the best LDA model?\n",
        ".\n",
        "The most important tuning parameter for LDA models is n_components (number of topics). In addition, I am going to search learning_decay (which controls the learning rate) as well.\n",
        "\n",
        "Besides these, other possible search params could be learning_offset (downweigh early iterations. Should be > 1) and max_iter. These could be worth experimenting if you have enough computing resources.\n",
        "\n",
        "Be warned, the grid search constructs multiple LDA models for all possible combinations of param values in the param_grid dict. So, this process can consume a lot of time and resources."
      ],
      "metadata": {
        "id": "eYFH2Wr5PSXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "metadata": {
        "id": "rUsTkHhgPIBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/darenr/scikit-optimize"
      ],
      "metadata": {
        "id": "a99mV6DwSTpw",
        "outputId": "db0802d1-a8a0-4c69-c910-e4a05e865dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/darenr/scikit-optimize\n",
            "  Cloning https://github.com/darenr/scikit-optimize to /tmp/pip-req-build-gs6jrw8_\n",
            "  Running command git clone -q https://github.com/darenr/scikit-optimize /tmp/pip-req-build-gs6jrw8_\n",
            "Collecting pyaml\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.6+19.g180d6be) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.6+19.g180d6be) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml->scikit-optimize==0.6+19.g180d6be) (3.13)\n",
            "Building wheels for collected packages: scikit-optimize\n",
            "  Building wheel for scikit-optimize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-optimize: filename=scikit_optimize-0.6+19.g180d6be-py2.py3-none-any.whl size=75418 sha256=fad3c6e31f5947b1201c51306c7a3f0ac6a9eadc8ea898078ed2c747d099d3b5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xdlr8d_s/wheels/11/4c/bf/aa91b51947ed5367f98d5a68c59afd45a275b85e9fc4827007\n",
            "Successfully built scikit-optimize\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.10.1 scikit-optimize-0.6+19.g180d6be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV(cv=None, error_score='raise',\n",
        "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
        "             evaluate_every=-1, learning_decay=0.7, learning_method='batch',\n",
        "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
        "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
        "             perp_tol=0.1, random_state=None,\n",
        "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
        "       fit_params=None, iid=True, n_jobs=1,\n",
        "       param_grid={'n_components': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
        "       scoring=None, verbose=0)"
      ],
      "metadata": {
        "id": "o8QSMLpkPl_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. How to see the best topic model and its parameters?"
      ],
      "metadata": {
        "id": "BjPmOUTUPn09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "metadata": {
        "id": "KiihYT78PrBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Compare LDA Model Performance Scores"
      ],
      "metadata": {
        "id": "I6i59T0cP2AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Log Likelyhoods from Grid Search Output\n",
        "n_topics = [10, 15, 20, 25, 30]\n",
        "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]\n",
        "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
        "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
        "\n",
        "# Show graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
        "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
        "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
        "plt.title(\"Choosing Optimal LDA Model\")\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Log Likelyhood Scores\")\n",
        "plt.legend(title='Learning decay', loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qKsOpkysPvJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. How to see the dominant topic in each document?"
      ],
      "metadata": {
        "id": "NpYlzOcHQAf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Document - Topic Matrix\n",
        "lda_output = best_lda_model.transform(data_vectorized)\n",
        "\n",
        "# column names\n",
        "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_topics)]\n",
        "\n",
        "# index names\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
        "\n",
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "# Styling\n",
        "def color_green(val):\n",
        "    color = 'green' if val > .1 else 'black'\n",
        "    return 'color: {col}'.format(col=color)\n",
        "\n",
        "def make_bold(val):\n",
        "    weight = 700 if val > .1 else 400\n",
        "    return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n",
        "# Apply Style\n",
        "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics"
      ],
      "metadata": {
        "id": "6prPcdliP8f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. Review topics distribution across documents"
      ],
      "metadata": {
        "id": "Y_mRkDc1y7hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
        "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
        "df_topic_distribution"
      ],
      "metadata": {
        "id": "yW-j9J4_y68V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. How to visualize the LDA model with pyLDAvis?"
      ],
      "metadata": {
        "id": "H2d8j4F0zoXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
        "panel"
      ],
      "metadata": {
        "id": "UVkOo5b_zrs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. How to see the Topic’s keywords?"
      ],
      "metadata": {
        "id": "Z_Ne0aEwz0h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic-Keyword Matrix\n",
        "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
        "\n",
        "# Assign Column and Index\n",
        "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
        "df_topic_keywords.index = topicnames\n",
        "\n",
        "# View\n",
        "df_topic_keywords.head()"
      ],
      "metadata": {
        "id": "Rieuyq2ez3R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. Get the top 15 keywords each topic\n",
        "\n",
        "From the above output, I want to see the top 15 keywords that are representative of the topic.\n",
        "\n",
        "The show_topics() defined below creates that."
      ],
      "metadata": {
        "id": "mBeXnSEj0WEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top n keywords for each topic\n",
        "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "\n",
        "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "df_topic_keywords"
      ],
      "metadata": {
        "id": "wkQxBP7y0ZF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. How to predict the topics for a new piece of text?\n",
        "Assuming that you have already built the topic model, you need to take the text through the same routine of transformations and before predicting the topic.\n",
        "\n",
        "For our case, the order of transformations is:"
      ],
      "metadata": {
        "id": "_lBdowYJ0gN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to predict topic for a given text document.\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def predict_topic(text, nlp=nlp):\n",
        "    global sent_to_words\n",
        "    global lemmatization\n",
        "\n",
        "    # Step 1: Clean with simple_preprocess\n",
        "    mytext_2 = list(sent_to_words(text))\n",
        "\n",
        "    # Step 2: Lemmatize\n",
        "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "    # Step 3: Vectorize transform\n",
        "    mytext_4 = vectorizer.transform(mytext_3)\n",
        "\n",
        "    # Step 4: LDA Transform\n",
        "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
        "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
        "    return topic, topic_probability_scores\n",
        "\n",
        "# Predict the topic\n",
        "mytext = [\"Some text about christianity and bible\"]\n",
        "topic, prob_scores = predict_topic(text = mytext)\n",
        "print(topic)"
      ],
      "metadata": {
        "id": "3dTf1mFT0fmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. How to cluster documents that share similar topics and plot?\n",
        "You can use k-means clustering on the document-topic probabilioty matrix, which is nothing but lda_output object. Since out best model has 15 clusters, I’ve set n_clusters=15 in KMeans().\n",
        "\n",
        "Alternately, you could avoid k-means and instead, assign the cluster as the topic column number with the highest probability score.\n",
        "\n",
        "We now have the cluster number. But we also need the X and Y columns to draw the plot.\n",
        "\n",
        "For the X and Y, you can use SVD on the lda_output object with n_components as 2. SVD ensures that these two columns captures the maximum possible amount of information from lda_output in the first 2 components.\n",
        "\n"
      ],
      "metadata": {
        "id": "XSx8aCMn0rKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the k-means clusters\n",
        "from sklearn.cluster import KMeans\n",
        "clusters = KMeans(n_clusters=15, random_state=100).fit_predict(lda_output)\n",
        "\n",
        "# Build the Singular Value Decomposition(SVD) model\n",
        "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
        "lda_output_svd = svd_model.fit_transform(lda_output)\n",
        "\n",
        "# X and Y axes of the plot using SVD decomposition\n",
        "x = lda_output_svd[:, 0]\n",
        "y = lda_output_svd[:, 1]\n",
        "\n",
        "# Weights for the 15 columns of lda_output, for each component\n",
        "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
        "\n",
        "# Percentage of total information in 'lda_output' explained by the two components\n",
        "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
      ],
      "metadata": {
        "id": "V6AiSiTY0q5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the X, Y and the cluster number for each document.\n",
        "\n",
        "Let’s plot the document along the two SVD decomposed components. The color of points represents the cluster number (in this case) or topic number."
      ],
      "metadata": {
        "id": "i5NsFCpb0zv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(x, y, c=clusters)\n",
        "plt.xlabel('Component 2')\n",
        "plt.xlabel('Component 1')\n",
        "plt.title(\"Segregation of Topic Clusters\", )"
      ],
      "metadata": {
        "id": "d6ZzGfRG0221"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. How to get similar documents for any given piece of text?\n",
        "Once you know the probaility of topics for a given document (using predict_topic()), compute the euclidean distance with the probability scores of all other documents.\n",
        "The most similar documents are the ones with the smallest distance."
      ],
      "metadata": {
        "id": "c9dDXUpZ07ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def similar_documents(text, doc_topic_probs, documents = data, nlp=nlp, top_n=5, verbose=False):\n",
        "    topic, x  = predict_topic(text)\n",
        "    dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]\n",
        "    doc_ids = np.argsort(dists)[:top_n]\n",
        "    if verbose:        \n",
        "        print(\"Topic KeyWords: \", topic)\n",
        "        print(\"Topic Prob Scores of text: \", np.round(x, 1))\n",
        "        print(\"Most Similar Doc's Probs:  \", np.round(doc_topic_probs[doc_ids], 1))\n",
        "    return doc_ids, np.take(documents, doc_ids)"
      ],
      "metadata": {
        "id": "dLwpnewM0_Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get similar documents\n",
        "mytext = [\"Some text about christianity and bible\"]\n",
        "doc_ids, docs = similar_documents(text=mytext, doc_topic_probs=lda_output, documents = data, top_n=1, verbose=True)\n",
        "print('\\n', docs[0][:500])"
      ],
      "metadata": {
        "id": "R9Cbksr71G1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. Conclusion\n",
        "We’ve covered some cutting-edge topic modeling approaches in this post.\n",
        "\n",
        "If you managed to work this through, well done.\n",
        "\n",
        "For those concerned about the time, memory consumption and variety of topics when building topic models check out the gensim tutorial on LDA.\n",
        "\n",
        "I will meet you with a new tutorial next week."
      ],
      "metadata": {
        "id": "PJaaBdYp1Pzz"
      }
    }
  ]
}